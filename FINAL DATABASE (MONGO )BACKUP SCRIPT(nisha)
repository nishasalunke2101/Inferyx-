       =================================FINAL RUNNING backupscript===================================


inferyx@wks-003:/install/framework/bin$ cat backupscript.sh
#!/bin/bash

# Function to show usage

usage() {
    echo "Usage: $0"
    echo "  -t <db_type>"
    echo "  -u <username>"
    echo "  -p <password>"
    echo "  -d <database_names>"
    echo "  -h <hostname>"
    echo "  -P <port>"
    echo "  -a <aws_snap(true/false)>"
    echo "  -b <backup_dir>"
    echo "  -s <s3_bucketname>"
    echo "  -r <retention_days>"
    exit 1
}

# Function to log errors
log_error() {
    echo "$(date +'%Y-%m-%d %H:%M:%S') - ERROR: $1" >> backup.log
}

# Function to create EC2 snapshot
create_ec2_snapshot() {
    if [ "$AWS_SNAP" == "true" ]; then
        INSTANCE_ID=$(curl -s http://169.254.169.254/latest/meta-data/instance-id)
        SNAPSHOT_ID=$(aws ec2 create-snapshot --description "EC2 snapshot on $CURRENT_DATE" --volume-id $INSTANCE_ID --query SnapshotId --output text)
        if [ -n "$SNAPSHOT_ID" ]; then
            echo "EC2 snapshot created: $SNAPSHOT_ID"
            echo $SNAPSHOT_ID > snapshot_id.txt  # Store snapshot ID in a file for later reference
        else
            log_error "EC2 snapshot creation failed"
            exit 1
        fi
    fi
}

# Function to check EC2 snapshot status
check_snapshot_status() {
    if [ -f "snapshot_id.txt" ]; then
        SNAPSHOT_ID=$(<snapshot_id.txt)
        STATUS=$(aws ec2 describe-snapshots --snapshot-ids $SNAPSHOT_ID --query 'Snapshots[*].State' --output text)
        echo "Snapshot status: $STATUS"
    else
        log_error "Snapshot ID file not found"
    fi
}

# Parse arguments
while getopts ":t:u:p:d:h:P:a:b:s:r:" opt; do
    case $opt in
        t) DB_TYPE="$OPTARG" ;;
        u) DB_USERNAME="$OPTARG" ;;
        p) DB_PASSWORD="$OPTARG" ;;
        d) DB_NAMES="$OPTARG" ;;  # Accept comma-separated list of database names
        h) DB_HOSTNAME="$OPTARG" ;;
        P) DB_PORT="$OPTARG" ;;
        a) AWS_SNAP="$OPTARG" ;;
        b) BACKUP_DIR="$OPTARG" ;;
        s) S3_BUCKET="$OPTARG" ;;
        r) RETENTION_DAYS="$OPTARG" ;;
        *) usage ;;
    esac
done

# Check for mandatory arguments
if [ -z "$DB_TYPE" ] || [ -z "$DB_USERNAME" ] || [ -z "$DB_PASSWORD" ] || [ -z "$DB_NAMES" ] || [ -z "$DB_HOSTNAME" ] || [ -z "$DB_PORT" ] || [ -z "$BACKUP_DIR" ] || [ -z "$S3_BUCKET" ] || [ -z "$RETENTION_DAYS" ]; then
    usage
fi

# Current date
CURRENT_DATE=$(date +"%Y-%m-%d")

# Database backup function
backup_database() {
    if [ "$DB_TYPE" == "mysql" ]; then
        IFS=',' read -ra DB_ARRAY <<< "$DB_NAMES"  # Split database names into an array
        for DB_NAME in "${DB_ARRAY[@]}"; do
            BACKUP_FILE="${BACKUP_DIR}/${DB_NAME}_${CURRENT_DATE}.sql.gz"
            mysqldump -h $DB_HOSTNAME -P $DB_PORT -u $DB_USERNAME -p$DB_PASSWORD $DB_NAME | gzip > $BACKUP_FILE
            if [ -f "$BACKUP_FILE" ]; then
                echo "Database backup successful: $BACKUP_FILE"
            else
                log_error "Database backup failed for $DB_NAME"
                exit 1
            fi
        done
    else
        case $DB_TYPE in
            mongodb)
               echo "Taking mongo backup.... $BACKUP_FILE "

                BACKUP_FILE="${BACKUP_DIR}/${DB_NAMES}_${CURRENT_DATE}.gz"
                mongodump --host $DB_HOSTNAME --port $DB_PORT --username $DB_USERNAME --password $DB_PASSWORD --authenticationDatabase admin  --db $DB_NAMES --archive=$BACKUP_FILE --gzip
		echo "Backup dir : $BACKUP_FILE "
		echo "Uploading on s3..."
		upload_to_s3 $BACKUP_FILE
               ;;
            snowflake)
                echo "Snowflake backup not implemented"
                ;;
            postgrase)
                BACKUP_FILE="${BACKUP_DIR}/${DB_NAMES}_${CURRENT_DATE}.sql.gz"
                PGPASSWORD=$DB_PASSWORD pg_dump -h $DB_HOSTNAME -p $DB_PORT -U $DB_USERNAME $DB_NAMES | gzip > $BACKUP_FILE
                ;;
            hive)
                echo "Hive backup not implemented"
                ;;
            apache-Iceberg)
                echo "Apache Iceberg backup not implemented"
                ;;
            presto)
                echo "Presto backup not implemented"
                ;;
            neo4j)
                BACKUP_FILE="${BACKUP_DIR}/${DB_NAMES}_${CURRENT_DATE}.dump"
                neo4j-admin dump --database=$DB_NAMES --to=$BACKUP_FILE
                ;;
            *)
                log_error "Unsupported database type: $DB_TYPE"
                exit 1
                ;;
        esac

        if [ -f "$BACKUP_FILE" ]; then
            echo "Database backup successful: $BACKUP_FILE"
        else
            log_error "Database backup failed"
            exit 1
        fi
    fi
}

# Application framework backup function
backup_app_framework() {
    APP_BACKUP_FILE="${BACKUP_DIR}/app_framework_${CURRENT_DATE}.tar.gz"
    tar -czvf $APP_BACKUP_FILE /app/framework
    if [ -f "$APP_BACKUP_FILE" ]; then
        echo "Application framework backup successful: $APP_BACKUP_FILE"
	upload_to_s3 $APP_BACKUP_FILE
    else
        log_error "Application framework backup failed"
        exit 1
    fi
}

# Function to check if S3 bucket exists
check_s3_bucket() {
    if aws s3 ls "s3://$S3_BUCKET" 2>&1 | grep -q 'NoSuchBucket'; then
        echo "S3 bucket '$S3_BUCKET' does not exist"
        exit 1
    else
        echo "S3 bucket '$S3_BUCKET' exists"
    fi
}


# Upload to S3
upload_to_s3() {
 local BACKUP_FILES=$1
 echo "DB backup dir: $BACKUP_FILES"
    aws s3 cp $BACKUP_FILES s3://inferyx/backup/
    if [ $? -eq 0 ]; then
        echo "Upload to S3 successful: $1"
    else
        log_error "Upload to S3 failed: $1"
        exit 1
    fi
}

# Remove old backups from S3
remove_old_backups() {
    # List objects in the S3 bucket recursively and filter out directories
    aws s3 ls s3://$S3_BUCKET/backup/ --recursive | while read -r line; do
        # Extract the filename
        FILE_NAME=$(echo $line | awk '{print $4}')
        # Check if the file name is not empty (i.e., it's a file, not a directory)
        if [ ! -z "$FILE_NAME" ]; then
            # Print the file name for debugging
            echo "File name: $FILE_NAME"
            # Delete the file from S3
#            aws s3 rm s3://$S3_BUCKET/$FILE_NAME
            # Check if deletion was successful
            if [ $? -eq 0 ]; then
                echo "Deleted $FILE_NAME from S3 bucket"
            else
                log_error "Failed to delete $FILE_NAME from S3 bucket"
            fi
        fi
    done
}



# Upload all database backups to S3
#IFS=',' read -ra DB_ARRAY <<< "$DB_NAMES"
#for DB_NAME in "${DB_ARRAY[@]}"; do
#    BACKUP_FILE="${BACKUP_DIR}/${DB_NAME}_${CURRENT_DATE}.sql.gz"
#    upload_to_s3 $BACKUP_FILE
#    upload_to_s3 $BACKUP_FILE

#    if [ $? -ne 0 ]; then
#        log_error "Upload of database backup $BACKUP_FILE to S3 failed"
#        exit 1
#    fi
#done

#upload_to_s3 $APP_BACKUP_FILE
#if [ $? -ne 0 ]; then
#    log_error "Upload of application framework backup to S3 failed"
 #   exit 1
#fi

#remove_old_backups
#if [ $? -ne 0 ]; then
#    log_error "Removal of old backups from S3 failed"
#    exit 1
#fi
# Main script execution
#backup_database
#backup_app_framework
#create_ec2_snapshot
remove_old_backups

echo "Backup script completed successfully"

